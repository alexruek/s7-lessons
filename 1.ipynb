{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bc9bde257068881",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T12:37:16.503650Z",
     "start_time": "2024-02-19T12:37:16.394505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/lib/python3.8/dist-packages/pyspark'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T12:37:22.668173Z",
     "start_time": "2024-02-19T12:37:22.452360Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", 2) \\\n",
    "    .config(\"spark.driver.cores\", 2) \\\n",
    "    .appName(\"Learning DataFrames\") \\\n",
    "    .getOrCreate()                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ce19e95b79115e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T06:52:20.487063Z",
     "start_time": "2024-02-19T06:52:20.375248Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88a87d7508a3f28f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T12:43:15.314994Z",
     "start_time": "2024-02-19T12:43:15.208384Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [('Max', 55),\n",
    "        ('Yan', 53),\n",
    "        ('Dmitry', 54),\n",
    "        ('Ann', 25)\n",
    "        ]\n",
    "\n",
    "columns = ['Name', 'Age']\n",
    "df = spark.createDataFrame(data=data, schema=columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "993159f434a9153e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T12:43:18.560568Z",
     "start_time": "2024-02-19T12:43:18.241525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|   Max| 55|\n",
      "|   Yan| 53|\n",
      "|Dmitry| 54|\n",
      "|   Ann| 25|\n",
      "+------+---+\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7028376b54157dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ab0545c995e53cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T12:43:20.164237Z",
     "start_time": "2024-02-19T12:43:20.024390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "867df463fb8605b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T19:44:14.569524Z",
     "start_time": "2024-02-20T19:44:13.942406Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3fcc9df8f2af5b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T12:55:35.713639Z",
     "start_time": "2024-02-19T12:55:19.541692Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2024-02-19 13:05:51,350 WARN util.Utils: Your hostname, fhmv3b8qjqqcv8g34hsf resolves to a loopback address: 127.0.1.1; using 172.16.0.22 instead (on interface eth0)\n",
      "2024-02-19 13:05:51,351 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-02-19 13:05:53,273 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "2024-02-19 13:05:59,393 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "2024-02-19 13:05:59,403 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-19 13:06:24.087120\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import datetime\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark import SparkConf\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"alexpa_4_2\") \\\n",
    "    .config(\"spark.ui.port\", \"4050\") \\\n",
    "    .config(\"spark.yarn.queue\", \"root\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8a4699d30805d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T12:47:35.613730Z",
     "start_time": "2024-02-19T12:47:34.936735Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 13:06:47,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:07:02,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:07:17,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:07:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:07:47,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:08:02,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:08:17,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:08:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:08:47,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:09:02,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:09:17,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:09:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:09:47,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:10:02,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:10:17,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:10:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:10:47,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:11:02,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:11:17,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:11:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:11:47,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:12:02,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:12:17,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:12:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:12:47,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:13:02,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:13:17,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:13:32,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:13:47,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:14:02,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:14:17,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:14:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:14:47,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:15:02,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:15:17,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:15:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:15:47,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:16:02,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:16:17,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:16:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:16:47,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:17:02,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:17:17,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:17:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:17:47,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:18:02,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:18:17,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:18:32,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:18:47,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:19:02,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:19:17,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:19:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:19:47,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:20:02,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:20:17,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:20:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:20:47,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:21:02,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:21:17,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:21:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:21:47,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:22:02,759 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:22:17,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2024-02-19 13:22:32,758 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|               event|  event_type|\n",
      "+--------------------+------------+\n",
      "|[,, 2022-05-25 17...|subscription|\n",
      "|[,, 2022-05-25 15...|subscription|\n",
      "|[,, 2022-05-25 18...|subscription|\n",
      "|[,, 2022-05-25 00...|subscription|\n",
      "|[,, 2022-05-25 03...|subscription|\n",
      "|[,, 2022-05-25 04...|subscription|\n",
      "|[,, 2022-05-25 15...|subscription|\n",
      "|[,, 2022-05-25 01...|subscription|\n",
      "|[,, 2022-05-25 00...|subscription|\n",
      "|[,, 2022-05-25 07...|subscription|\n",
      "+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events = spark.read.load(path = \"/user/master/data/events/date=2022-05-25\", format = 'json') \n",
    "events.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ebcd141a0882f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   5 hdfs hadoop          0 2023-09-25 20:34 /user/master/data/snapshots/channels/actual/_SUCCESS\r\n",
      "-rw-r--r--   5 hdfs hadoop    4295876 2023-09-25 20:34 /user/master/data/snapshots/channels/actual/part-00000-beae86b3-af54-4415-8f29-31dd79cfe178-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/master/data/snapshots/channels/actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "089a56e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/user/master/data/snapshots/channels/actual/part-00000-beae86b3-af54-4415-8f29-31dd79cfe178-c000.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0512f125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"channel_type\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .parquet(\"/user/timefor/analytics/test\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a07524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   1 timefor timefor          0 2024-02-19 13:41 /user/timefor/analytics/test/_SUCCESS\r\n",
      "drwxr-xr-x   - timefor timefor          0 2024-02-19 13:41 /user/timefor/analytics/test/channel_type=channel\r\n",
      "drwxr-xr-x   - timefor timefor          0 2024-02-19 13:41 /user/timefor/analytics/test/channel_type=river\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/timefor/analytics/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aee23a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|channel_type|\n",
      "+------------+\n",
      "|       river|\n",
      "|     channel|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.parquet(\"/user/timefor/analytics/test\").select(\"channel_type\").orderBy(\"channel_type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3cfa4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e051ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName(\"Learning DataFrames\") \\\n",
    "                    .getOrCreate()\n",
    "# данные первого датафрейма \n",
    "book = [('Harry Potter and the Goblet of Fire', 'J. K. Rowling', 322),\n",
    "        ('Nineteen Eighty-Four', 'George Orwell', 382),\n",
    "        ('Jane Eyre', 'Charlotte Brontë', 159),\n",
    "        ('Catch-22', 'Joseph Heller',  174),\n",
    "        ('The Catcher in the Rye', 'J. D. Salinger',  168),\n",
    "        ('The Wind in the Willows', 'Kenneth Grahame',  259),\n",
    "        ('The Mayor of Casterbridge', 'Thomas Hardy',  300),\n",
    "        ('Bad Girls', 'Jacqueline Wilson',  299)\n",
    "]\n",
    "# данные второго датафрейма\n",
    "library = [\n",
    "        ( 322, \"1\"),\n",
    "        ( 250, \"2\" ),\n",
    "        (400, \"2\"),\n",
    "        (159, \"1\"),\n",
    "        (382, \"2\"),\n",
    "        (322, \"1\")\n",
    "]\n",
    "# названия атрибутов\n",
    "columns = ['title', 'author', 'book_id']\n",
    "columns_library = ['book_id', 'Library_id']\n",
    "# создаём датафреймы\n",
    "df = spark.createDataFrame(data=book, schema=columns)\n",
    "df_library  = spark.createDataFrame(data=library, schema=columns_library )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dbb45e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:23:42.312006Z",
     "start_time": "2024-02-19T14:23:42.208170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-------+\n",
      "|               title|           author|book_id|\n",
      "+--------------------+-----------------+-------+\n",
      "|Harry Potter and ...|    J. K. Rowling|    322|\n",
      "|Nineteen Eighty-Four|    George Orwell|    382|\n",
      "|           Jane Eyre| Charlotte Brontë|    159|\n",
      "|            Catch-22|    Joseph Heller|    174|\n",
      "|The Catcher in th...|   J. D. Salinger|    168|\n",
      "|The Wind in the W...|  Kenneth Grahame|    259|\n",
      "|The Mayor of Cast...|     Thomas Hardy|    300|\n",
      "|           Bad Girls|Jacqueline Wilson|    299|\n",
      "+--------------------+-----------------+-------+\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2d7f83d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:26:13.935277Z",
     "start_time": "2024-02-19T14:26:13.832281Z"
    }
   },
   "outputs": [],
   "source": [
    "joined = df.join(df_library, df.book_id == df_library.book_id, how='leftanti').drop(df.book_id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2f857a5e8cbcb3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T17:53:31.044909Z",
     "start_time": "2024-02-19T17:53:28.481310Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|title                    |\n",
      "+-------------------------+\n",
      "|The Mayor of Casterbridge|\n",
      "|Catch-22                 |\n",
      "|The Catcher in the Rye   |\n",
      "|The Wind in the Willows  |\n",
      "|Bad Girls                |\n",
      "+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "joined.select('title').distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "860d9f46b15c05cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T17:54:21.097031Z",
     "start_time": "2024-02-19T17:54:18.170535Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.select('title').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0e78a28e267e1ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T17:56:34.037579Z",
     "start_time": "2024-02-19T17:56:32.624979Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|title                              |\n",
      "+-----------------------------------+\n",
      "|Bad Girls                          |\n",
      "|Catch-22                           |\n",
      "|The Catcher in the Rye             |\n",
      "|Nineteen Eighty-Four               |\n",
      "|Harry Potter and the Goblet of Fire|\n",
      "|Harry Potter and the Goblet of Fire|\n",
      "|Jane Eyre                          |\n",
      "|The Mayor of Casterbridge          |\n",
      "|The Wind in the Willows            |\n",
      "+-----------------------------------+\n"
     ]
    }
   ],
   "source": [
    "df.join(df_library, df.book_id == df_library.book_id, how='left') \\\n",
    "    .drop(df.book_id ) \\\n",
    "    .select('title') \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c35c7f0448a811a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T17:59:58.244447Z",
     "start_time": "2024-02-19T17:59:58.134167Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Learning DataFrames\") \\\n",
    "    .getOrCreate()\n",
    "# данные первого датафрейма \n",
    "book_1 = [('Harry Potter and the Goblet of Fire', 'J. K. Rowling', 322),\n",
    "          ('Nineteen Eighty-Four', 'George Orwell', 382),\n",
    "          ('Jane Eyre', 'Charlotte Brontë', 159),\n",
    "          ('Catch-22', 'Joseph Heller',  174),\n",
    "          ('The Catcher in the Rye', 'J. D. Salinger',  168),\n",
    "          ('The Wind in the Willows', 'Kenneth Grahame',  259),\n",
    "          ('The Mayor of Casterbridge', 'Thomas Hardy',  300),\n",
    "          ('Bad Girls', 'Jacqueline Wilson',  299)\n",
    "          ]\n",
    "# данные второго датафрейма\n",
    "book_2 = [\n",
    "    ('Black Beauty',657 ,'Anna Sewell'),\n",
    "    ('Artemis Fowl',558,'Eoin Colfer'),\n",
    "    ('The Magic Faraway Tree', 567,'Enid Blyton'),\n",
    "    ('The Witches', 567,'Roald Dahl'),\n",
    "    ('Frankenstein',567 ,'Mary Shelley'),\n",
    "    ('The Little Prince',557 ,'Antoine de Saint-Exupéry'),\n",
    "    ('The Truth', 576 ,'Terry Pratchett')\n",
    "]\n",
    "# названия атрибутов\n",
    "columns_1= ['title', 'author', 'book_id']\n",
    "columns_2 = ['title', 'book_id', 'author']\n",
    "# создаём датафреймы\n",
    "df_1 = spark.createDataFrame(data=book_1 , schema=columns_1)\n",
    "df_2  = spark.createDataFrame(data=book_2 , schema=columns_2)\n",
    "# напишите ваш код ниже\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15bb1d81a60078d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T18:01:35.259149Z",
     "start_time": "2024-02-19T18:01:34.818644Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               title|              author|book_id|\n",
      "+--------------------+--------------------+-------+\n",
      "|Harry Potter and ...|       J. K. Rowling|    322|\n",
      "|Nineteen Eighty-Four|       George Orwell|    382|\n",
      "|           Jane Eyre|    Charlotte Brontë|    159|\n",
      "|            Catch-22|       Joseph Heller|    174|\n",
      "|The Catcher in th...|      J. D. Salinger|    168|\n",
      "|The Wind in the W...|     Kenneth Grahame|    259|\n",
      "|The Mayor of Cast...|        Thomas Hardy|    300|\n",
      "|           Bad Girls|   Jacqueline Wilson|    299|\n",
      "|        Black Beauty|         Anna Sewell|    657|\n",
      "|        Artemis Fowl|         Eoin Colfer|    558|\n",
      "|The Magic Faraway...|         Enid Blyton|    567|\n",
      "|         The Witches|          Roald Dahl|    567|\n",
      "|        Frankenstein|        Mary Shelley|    567|\n",
      "|   The Little Prince|Antoine de Saint-...|    557|\n",
      "|           The Truth|     Terry Pratchett|    576|\n",
      "+--------------------+--------------------+-------+\n"
     ]
    }
   ],
   "source": [
    "df_1.unionByName(df_2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83f0b2702f94c121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T18:10:56.920480Z",
     "start_time": "2024-02-19T18:10:56.819400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[title: string, author: string]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c62bee2701d72a0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T18:11:17.547195Z",
     "start_time": "2024-02-19T18:11:17.352336Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [title#66, author#67]\n",
      "   +- InMemoryRelation [title#66, author#67], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(5) Project [title#66, author#67]\n",
      "            +- SortMergeJoin [book_id#68L], [book_id#72L], LeftAnti\n",
      "               :- *(2) Sort [book_id#68L ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(book_id#68L, 200), true, [id=#640]\n",
      "               :     +- *(1) Scan ExistingRDD[title#66,author#67,book_id#68L]\n",
      "               +- *(4) Sort [book_id#72L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(book_id#72L, 200), true, [id=#645]\n",
      "                     +- *(3) Project [book_id#72L]\n",
      "                        +- *(3) Filter isnotnull(book_id#72L)\n",
      "                           +- *(3) Scan ExistingRDD[book_id#72L,Library_id#73]\n"
     ]
    }
   ],
   "source": [
    "joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e564eec06eb8835c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T18:14:49.542784Z",
     "start_time": "2024-02-19T18:14:34.273246Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[title: string, author: string]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.setCheckpointDir(\"/user/timefor/analytics/test_check\")\n",
    "joined.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "451e4ac64d1660b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T18:14:49.640858Z",
     "start_time": "2024-02-19T18:14:49.542595Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [title#66, author#67]\n",
      "   +- InMemoryRelation [title#66, author#67], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(5) Project [title#66, author#67]\n",
      "            +- SortMergeJoin [book_id#68L], [book_id#72L], LeftAnti\n",
      "               :- *(2) Sort [book_id#68L ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(book_id#68L, 200), true, [id=#640]\n",
      "               :     +- *(1) Scan ExistingRDD[title#66,author#67,book_id#68L]\n",
      "               +- *(4) Sort [book_id#72L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(book_id#72L, 200), true, [id=#645]\n",
      "                     +- *(3) Project [book_id#72L]\n",
      "                        +- *(3) Filter isnotnull(book_id#72L)\n",
      "                           +- *(3) Scan ExistingRDD[book_id#72L,Library_id#73]\n"
     ]
    }
   ],
   "source": [
    "joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29b3a0ca1052b71e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T18:17:05.426031Z",
     "start_time": "2024-02-19T18:16:45.673595Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Learning DataFrames\") \\\n",
    "    .getOrCreate()\n",
    "# данные первого датафрейма\n",
    "\n",
    "book = [('Harry Potter and the Goblet of Fire', 'J. K. Rowling', 322),\n",
    "        ('Nineteen Eighty-Four', 'George Orwell', 382),\n",
    "        ('Jane Eyre', 'Charlotte Brontë', 159),\n",
    "        ('Catch-22', 'Joseph Heller',  174),\n",
    "        ('The Catcher in the Rye', 'J. D. Salinger',  168),\n",
    "        ('The Wind in the Willows', 'Kenneth Grahame',  259),\n",
    "        ('The Mayor of Casterbridge', 'Thomas Hardy',  300),\n",
    "        ('Bad Girls', 'Jacqueline Wilson',  299)\n",
    "        ]\n",
    "\n",
    "# данные второго датафрейма\n",
    "library = [\n",
    "    ( 322, \"1\"),\n",
    "    ( 250, \"2\" ),\n",
    "    (400, \"2\"),\n",
    "    (159, \"1\"),\n",
    "    (382, \"2\"),\n",
    "    (322, \"1\")\n",
    "]\n",
    "\n",
    "# названия атрибутов\n",
    "columns = ['title', 'author', 'book_id']\n",
    "columns_library = ['book_id', 'Library_id']\n",
    "\n",
    "# создаём датафреймы\n",
    "df = spark.createDataFrame(data=book, schema=columns)\n",
    "df_library  = spark.createDataFrame(data=library, schema=columns_library )\n",
    "\n",
    "# делаем join\n",
    "df_join = df.join(df_library,['book_id'], 'leftanti').select('title')\n",
    "df_cache= df_join.cache()\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(\"/user/timefor/analytics/test_check\")\n",
    "df_checkpoint=df_join.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f18da09454bb7a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T18:17:18.484059Z",
     "start_time": "2024-02-19T18:17:18.372416Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [title#264]\n",
      "   +- InMemoryRelation [title#264], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(5) Project [title#264]\n",
      "            +- SortMergeJoin [book_id#266L], [book_id#270L], LeftAnti\n",
      "               :- *(2) Sort [book_id#266L ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(book_id#266L, 200), true, [id=#691]\n",
      "               :     +- *(1) Project [title#264, book_id#266L]\n",
      "               :        +- *(1) Scan ExistingRDD[title#264,author#265,book_id#266L]\n",
      "               +- *(4) Sort [book_id#270L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(book_id#270L, 200), true, [id=#696]\n",
      "                     +- *(3) Project [book_id#270L]\n",
      "                        +- *(3) Filter isnotnull(book_id#270L)\n",
      "                           +- *(3) Scan ExistingRDD[book_id#270L,Library_id#271]\n"
     ]
    }
   ],
   "source": [
    "df_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94d1956f55c3b95f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T18:23:10.811653Z",
     "start_time": "2024-02-19T18:23:09.958030Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af594285afe3b8e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T18:30:28.226390Z",
     "start_time": "2024-02-19T18:23:19.243032Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+\n",
      "|               event|  event_type|      date|\n",
      "+--------------------+------------+----------+\n",
      "|[,, 2022-05-25 17...|subscription|2022-05-25|\n",
      "|[,, 2022-05-25 15...|subscription|2022-05-25|\n",
      "|[,, 2022-05-25 18...|subscription|2022-05-25|\n",
      "|[,, 2022-05-25 00...|subscription|2022-05-25|\n",
      "|[,, 2022-05-25 03...|subscription|2022-05-25|\n",
      "|[,, 2022-05-25 04...|subscription|2022-05-25|\n",
      "|[,, 2022-05-25 15...|subscription|2022-05-25|\n",
      "|[,, 2022-05-25 01...|subscription|2022-05-25|\n",
      "|[,, 2022-05-25 00...|subscription|2022-05-25|\n",
      "|[,, 2022-05-25 07...|subscription|2022-05-25|\n",
      "+--------------------+------------+----------+\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Learning DataFrames\") \\\n",
    "    .getOrCreate()\n",
    "events = spark.read.json(\"/user/master/data/events/\")\n",
    "events.show(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f21c93ff7e05a1af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T18:31:09.857360Z",
     "start_time": "2024-02-19T18:31:09.504685Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+------------+----------+------------+\n",
      "|event                                              |event_type  |date      |current_date|\n",
      "+---------------------------------------------------+------------+----------+------------+\n",
      "|[,, 2022-05-25 17:04:31,,,,,,,,,,, 913681,,, 48430]|subscription|2022-05-25|2024-02-19  |\n",
      "|[,, 2022-05-25 15:20:12,,,,,,,,,,, 579406,,, 84256]|subscription|2022-05-25|2024-02-19  |\n",
      "|[,, 2022-05-25 18:49:53,,,,,,,,,,, 84145,,, 48457] |subscription|2022-05-25|2024-02-19  |\n",
      "|[,, 2022-05-25 00:15:15,,,,,,,,,,, 344598,,, 84308]|subscription|2022-05-25|2024-02-19  |\n",
      "|[,, 2022-05-25 03:20:22,,,,,,,,,,, 596627,,, 48474]|subscription|2022-05-25|2024-02-19  |\n",
      "|[,, 2022-05-25 04:54:08,,,,,,,,,,, 767711,,, 62234]|subscription|2022-05-25|2024-02-19  |\n",
      "|[,, 2022-05-25 15:39:11,,,,,,,,,,, 657579,,, 69692]|subscription|2022-05-25|2024-02-19  |\n",
      "|[,, 2022-05-25 01:12:32,,,,,,,,,,, 822105,,, 62257]|subscription|2022-05-25|2024-02-19  |\n",
      "|[,, 2022-05-25 00:37:22,,,,,,,,,,, 88418,,, 69703] |subscription|2022-05-25|2024-02-19  |\n",
      "|[,, 2022-05-25 07:25:22,,,,,,,,,,, 875433,,, 62259]|subscription|2022-05-25|2024-02-19  |\n",
      "+---------------------------------------------------+------------+----------+------------+\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "events_curr_day = events.withColumn('current_date',F.current_date())\n",
    "events_curr_day.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6997cb391324b39a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T18:31:57.192288Z",
     "start_time": "2024-02-19T18:31:56.821288Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------+------------+----+\n",
      "|               event|  event_type|      date|current_date|diff|\n",
      "+--------------------+------------+----------+------------+----+\n",
      "|[,, 2022-05-25 17...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 15...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 18...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 00...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 03...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 04...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 15...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 01...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 00...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 07...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 13...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 22...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 11...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 15...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 22...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 17...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 02...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 01...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 09...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "|[,, 2022-05-25 18...|subscription|2022-05-31|  2024-02-19| 629|\n",
      "+--------------------+------------+----------+------------+----+\n"
     ]
    }
   ],
   "source": [
    "events_diff = events_curr_day \\\n",
    "    .withColumn('date', F.lit('2022-05-31')) \\\n",
    "    .withColumn('diff',F.datediff(F.col('current_date'),F.col('date'))) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e94a00fd42ec52fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T18:33:37.038404Z",
     "start_time": "2024-02-19T18:33:35.436423Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "events = spark.read.json(\"/user/master/data/events/date=2022-05-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d37216c012f5984",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T19:19:11.036032Z",
     "start_time": "2024-02-19T19:19:10.893992Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+------------+\n",
      "|event                                            |event_type  |\n",
      "+-------------------------------------------------+------------+\n",
      "|[,, 2022-05-31 06:03:31,,,,,,,,, 966719,, 100029]|subscription|\n",
      "|[,, 2022-05-31 07:14:34,,,,,,,,, 636643,, 32034] |subscription|\n",
      "|[,, 2022-05-31 00:42:22,,,,,,,,, 724091,, 83285] |subscription|\n",
      "|[,, 2022-05-31 16:58:26,,,,,,,,, 294429,, 54960] |subscription|\n",
      "|[,, 2022-05-31 00:57:03,,,,,,,,, 289857,, 8335]  |subscription|\n",
      "|[,, 2022-05-31 04:44:44,,,,,,,,, 246671,, 54975] |subscription|\n",
      "|[,, 2022-05-31 01:59:08,,,,,,,,, 708394,, 100411]|subscription|\n",
      "|[,, 2022-05-31 02:07:53,,,,,,,,, 24230,, 32085]  |subscription|\n",
      "|[,, 2022-05-31 06:23:12,,,,,,,,, 28869,, 83526]  |subscription|\n",
      "|[,, 2022-05-31 15:26:41,,,,,,,,, 294429,, 5523]  |subscription|\n",
      "|[,, 2022-05-31 00:00:02,,,,,,,,, 433766,, 100621]|subscription|\n",
      "|[,, 2022-05-31 10:45:23,,,,,,,,, 884358,, 32389] |subscription|\n",
      "|[,, 2022-05-31 03:06:37,,,,,,,,, 151648,, 100942]|subscription|\n",
      "|[,, 2022-05-31 00:50:18,,,,,,,,, 694215,, 55335] |subscription|\n",
      "|[,, 2022-05-31 08:51:12,,,,,,,,, 28869,, 101071] |subscription|\n",
      "|[,, 2022-05-31 00:23:16,,,,,,,,, 769054,, 55335] |subscription|\n",
      "|[,, 2022-05-31 00:22:34,,,,,,,,, 151648,, 84127] |subscription|\n",
      "|[,, 2022-05-31 16:51:49,,,,,,,,, 707846,, 32545] |subscription|\n",
      "|[,, 2022-05-31 09:33:36,,,,,,,,, 667479,, 101197]|subscription|\n",
      "|[,, 2022-05-31 01:37:55,,,,,,,,, 621631,, 55509] |subscription|\n",
      "+-------------------------------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "events.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4b68c0b7e55c96b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T19:20:09.024006Z",
     "start_time": "2024-02-19T19:20:08.916221Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events_with_hms = events \\\n",
    "    .withColumn('hour', F.hour(F.col('event.datetime'))) \\\n",
    "    .withColumn('minute', F.minute(F.col('event.datetime'))) \\\n",
    "    .withColumn('second', F.second(F.col('event.datetime'))) \\\n",
    "    .sort(F.col('event.datetime').desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "788f7d92726b9d2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T19:22:21.850163Z",
     "start_time": "2024-02-19T19:22:20.648113Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----+------+------+\n",
      "|               event|  event_type|hour|minute|second|\n",
      "+--------------------+------------+----+------+------+\n",
      "|[[19342], 987160,...|     message|  23|    57|    38|\n",
      "|[,, 2022-05-31 23...|subscription|  23|    56|    58|\n",
      "|[[26358], 247511,...|     message|  23|    56|    53|\n",
      "|[[79792], 748847,...|     message|  23|    56|    27|\n",
      "|[,, 2022-05-31 23...|subscription|  23|    53|    42|\n",
      "|[,, 2022-05-31 23...|subscription|  23|    53|    14|\n",
      "|[[151897], 396845...|     message|  23|    52|    15|\n",
      "|[,, 2022-05-31 23...|subscription|  23|    52|     8|\n",
      "|[,, 2022-05-31 23...|subscription|  23|    51|    19|\n",
      "|[,, 2022-05-31 23...|subscription|  23|    51|    15|\n",
      "+--------------------+------------+----+------+------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "events_with_hms.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3a701c7a6ae73fcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T19:23:59.353085Z",
     "start_time": "2024-02-19T19:23:58.339919Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "227667"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.filter(F.col('event.message_from').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "574d7cafb58813db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T19:25:38.636728Z",
     "start_time": "2024-02-19T19:25:37.719662Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.filter(F.col('event.message_to').isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f485aea02635c101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T19:27:24.722119Z",
     "start_time": "2024-02-19T19:27:23.367613Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227667"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.count() - events.na.drop(subset='event.message_from').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2bf1c93024ff8a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T19:27:48.232905Z",
     "start_time": "2024-02-19T19:27:46.898635Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|message_from|count|\n",
      "+------------+-----+\n",
      "|       77947|    1|\n",
      "|      142511|    1|\n",
      "|      153557|    1|\n",
      "|       99185|    1|\n",
      "|      160744|    1|\n",
      "|       24880|    2|\n",
      "|       51165|    1|\n",
      "|       38941|    1|\n",
      "|       51349|    1|\n",
      "|       51534|    1|\n",
      "|      126842|    1|\n",
      "|      159521|    1|\n",
      "|       91431|    1|\n",
      "|       87917|    1|\n",
      "|       27813|    1|\n",
      "|       33851|    1|\n",
      "|      166941|    1|\n",
      "|      100323|    1|\n",
      "|       22052|    1|\n",
      "|       75949|    1|\n",
      "+------------+-----+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "events.filter(F.col('event_type')=='message').groupBy(F.col('event.message_from')).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "84de2193470e7dbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T19:29:44.961730Z",
     "start_time": "2024-02-19T19:29:37.767558Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "events_1 = spark.read.json(\"/user/master/data/events/date=2022-05-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dfe31a7601512e79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T19:34:04.039335Z",
     "start_time": "2024-02-19T19:33:58.095893Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:===========================================>          (161 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|max_reactions|\n",
      "+-------------+\n",
      "|           11|\n",
      "+-------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "max_reactions = events_1 \\\n",
    "    .filter(F.col('event_type') == 'reaction') \\\n",
    "    .groupBy(F.col('event.reaction_from')) \\\n",
    "    .count() \\\n",
    "    .agg(F.max('count').alias('max_reactions'))\n",
    "\n",
    "max_reactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034ea83fd7d1317",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "9. Оконные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5405f6bf28a19e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T16:40:03.758826Z",
     "start_time": "2024-02-20T16:40:03.638067Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/lib/python3.8/dist-packages/pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf9d7d2651cd7d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T18:36:35.643171Z",
     "start_time": "2024-02-20T18:36:35.441821Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Learning DataFrames\") \\\n",
    "    .getOrCreate()\n",
    "# данные  датафрейма \n",
    "data = [('2021-01-06', 3744, 63, 322),\n",
    "        ('2021-01-04', 2434, 21, 382),\n",
    "        ('2021-01-04', 2434, 32, 159),\n",
    "        ('2021-01-05', 3744, 32, 159),\n",
    "        ('2021-01-06', 4342, 32, 159),\n",
    "        ('2021-01-05', 4342, 12, 259),\n",
    "        ('2021-01-06', 5677, 12, 259),\n",
    "        ('2021-01-04', 5677, 23, 499)\n",
    "        ]\n",
    "# названия атрибутов\n",
    "columns = ['dt', 'user_id', 'product_id', 'purchase_amount']\n",
    "# создаём датафрейм\n",
    "df = spark.createDataFrame(data=data, schema=columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e4383a58d02053a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T18:36:54.024300Z",
     "start_time": "2024-02-20T18:36:49.304361Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+---------------+\n",
      "|        dt|user_id|rank|purchase_amount|\n",
      "+----------+-------+----+---------------+\n",
      "|2021-01-04|   2434|   1|            159|\n",
      "|2021-01-04|   2434|   2|            382|\n",
      "|2021-01-04|   5677|   3|            499|\n",
      "|2021-01-05|   3744|   1|            159|\n",
      "|2021-01-05|   4342|   2|            259|\n",
      "|2021-01-06|   4342|   1|            159|\n",
      "|2021-01-06|   5677|   2|            259|\n",
      "|2021-01-06|   3744|   3|            322|\n",
      "+----------+-------+----+---------------+\n"
     ]
    }
   ],
   "source": [
    "# импортируем оконную функцию и модуль Spark Functions\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# создаём объект оконной функции\n",
    "window = Window().partitionBy(['dt']).orderBy(F.asc('purchase_amount'))\n",
    "\n",
    "# создаём колонку с рассчитанной статистикой по оконной функции\n",
    "df_window = df.withColumn(\"rank\", F.row_number().over(window))\n",
    "\n",
    "# выводим нужные колонки\n",
    "df_window.select('dt', 'user_id', 'rank', 'purchase_amount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d8634d350772a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T18:36:44.780229Z",
     "start_time": "2024-02-20T18:36:44.677692Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Window' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# создаем объект оконной функции\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m window \u001b[38;5;241m=\u001b[39m \u001b[43mWindow\u001b[49m\u001b[38;5;241m.\u001b[39morderBy(F\u001b[38;5;241m.\u001b[39masc(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurchase_amount\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# создаем колонку с рассчитанной статистикой по оконной функции\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df_window \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mrow_number()\u001b[38;5;241m.\u001b[39mover(window))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Window' is not defined"
     ]
    }
   ],
   "source": [
    "# создаем объект оконной функции\n",
    "window = Window.orderBy(F.asc('purchase_amount'))\n",
    "\n",
    "# создаем колонку с рассчитанной статистикой по оконной функции\n",
    "df_window = df.withColumn(\"row_number\", F.row_number().over(window))\n",
    "\n",
    "# выводим нужные колонки\n",
    "df_window.select('dt', 'user_id', 'purchase_amount', 'row_number').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b724d16d6f2f8f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T17:25:12.333174Z",
     "start_time": "2024-02-20T17:25:11.946567Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------------+-----+\n",
      "|        dt|user_id|purchase_amount|lag_1|\n",
      "+----------+-------+---------------+-----+\n",
      "|2021-01-04|   2434|            382| NULL|\n",
      "|2021-01-04|   2434|            159|  382|\n",
      "|2021-01-05|   3744|            159| NULL|\n",
      "|2021-01-06|   3744|            322|  159|\n",
      "|2021-01-05|   4342|            259| NULL|\n",
      "|2021-01-06|   4342|            159|  259|\n",
      "|2021-01-04|   5677|            499| NULL|\n",
      "|2021-01-06|   5677|            259|  499|\n",
      "+----------+-------+---------------+-----+\n"
     ]
    }
   ],
   "source": [
    "# создаём объект оконной функции\n",
    "window = Window().partitionBy('user_id').orderBy('dt')\n",
    "\n",
    "# создаём колонку с рассчитанной статистикой по оконной функции\n",
    "dfWithLag = df.withColumn(\"lag_1\",F.lag(\"purchase_amount\", 1).over(window))\n",
    "\n",
    "# фильтруем данные\n",
    "dfWithLag .select('dt','user_id', 'purchase_amount','lag_1').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "541e5ce302717845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T17:32:22.635489Z",
     "start_time": "2024-02-20T17:32:22.335897Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5465987acae3ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T18:20:03.868493Z",
     "start_time": "2024-02-20T17:41:26.383385Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/20 17:41:28 WARN Utils: Your hostname, fhmbmgti0ul88u8innkd resolves to a loopback address: 127.0.1.1; using 172.16.0.39 instead (on interface eth0)\n",
      "24/02/20 17:41:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/20 17:41:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/20 17:41:31 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/02/20 18:20:02 ERROR YarnClientSchedulerBackend: The YARN application has already ended! It might have been killed or the Application Master may have failed to start. Check the YARN application logs for more details.\n",
      "24/02/20 18:20:02 ERROR SparkContext: Error initializing SparkContext.\n",
      "org.apache.spark.SparkException: Application application_1692104774102_32547 was killed by user dr.who\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:98)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:65)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:604)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/02/20 18:20:02 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to send shutdown message before the AM has registered!\n",
      "24/02/20 18:20:02 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "24/02/20 18:20:02 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Application application_1692104774102_32547 was killed by user dr.who\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:98)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:65)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:604)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf\n\u001b[0;32m---> 14\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myarn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malexpa_9_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.ui.port\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4050\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.yarn.queue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Application application_1692104774102_32547 was killed by user dr.who\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:98)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:65)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:604)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark import SparkConf\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"alexpa_9_2\") \\\n",
    "    .config(\"spark.ui.port\", \"4050\") \\\n",
    "    .config(\"spark.yarn.queue\", \"root\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c7b17d77b25de",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-20T18:20:03.865010Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events = spark.read.json(\"/user/master/data/events/date=2022-05-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9b21be5677e9a72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T18:37:47.932370Z",
     "start_time": "2024-02-20T18:37:47.266177Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+---------------+-----+---+\n",
      "|        dt|user_id|product_id|purchase_amount|  avg|sum|\n",
      "+----------+-------+----------+---------------+-----+---+\n",
      "|2021-01-04|   2434|        21|            382|270.5|541|\n",
      "|2021-01-04|   2434|        32|            159|270.5|541|\n",
      "|2021-01-06|   3744|        63|            322|240.5|481|\n",
      "|2021-01-05|   3744|        32|            159|240.5|481|\n",
      "|2021-01-06|   4342|        32|            159|209.0|418|\n",
      "|2021-01-05|   4342|        12|            259|209.0|418|\n",
      "|2021-01-06|   5677|        12|            259|379.0|758|\n",
      "|2021-01-04|   5677|        23|            499|379.0|758|\n",
      "+----------+-------+----------+---------------+-----+---+\n"
     ]
    }
   ],
   "source": [
    "window = Window().partitionBy('user_id')\n",
    "\n",
    "df_window_agg= df.withColumn(\"avg\",F.avg(\"purchase_amount\").over(window)) \\\n",
    "    .withColumn(\"sum\",F.sum(\"purchase_amount\").over(window))\n",
    "df_window_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c37fbb1bf8037aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T18:44:11.028159Z",
     "start_time": "2024-02-20T18:44:10.306443Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+---------------+---+---+\n",
      "|        dt|user_id|product_id|purchase_amount|max|min|\n",
      "+----------+-------+----------+---------------+---+---+\n",
      "|2021-01-04|   2434|        21|            382|382|159|\n",
      "|2021-01-04|   2434|        32|            159|382|159|\n",
      "|2021-01-06|   3744|        63|            322|322|159|\n",
      "|2021-01-05|   3744|        32|            159|322|159|\n",
      "|2021-01-06|   4342|        32|            159|259|159|\n",
      "|2021-01-05|   4342|        12|            259|259|159|\n",
      "|2021-01-06|   5677|        12|            259|499|259|\n",
      "|2021-01-04|   5677|        23|            499|499|259|\n",
      "+----------+-------+----------+---------------+---+---+\n"
     ]
    }
   ],
   "source": [
    "window = Window().partitionBy('user_id')\n",
    "\n",
    "df_window_m= df.withColumn(\"max\",F.max(\"purchase_amount\").over(window)) \\\n",
    "    .withColumn(\"min\",F.min(\"purchase_amount\").over(window))\n",
    "df_window_m.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c7217f8b41b94f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T18:45:55.632883Z",
     "start_time": "2024-02-20T18:45:55.122214Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+\n",
      "|user_id|max|min|\n",
      "+-------+---+---+\n",
      "|   2434|382|159|\n",
      "|   4342|259|159|\n",
      "|   5677|499|259|\n",
      "|   3744|322|159|\n",
      "+-------+---+---+\n"
     ]
    }
   ],
   "source": [
    "# Группировка по user_id и агрегация для нахождения максимальной и минимальной суммы покупки\n",
    "df_grouped = df.groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "    F.max(\"purchase_amount\").alias(\"max\"),\n",
    "    F.min(\"purchase_amount\").alias(\"min\")\n",
    ")\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbceb9f46d3040f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ce7cd8d9d05566",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
