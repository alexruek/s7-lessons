{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d17284e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2024-02-20 18:09:41,507 WARN util.Utils: Your hostname, fhmbmgti0ul88u8innkd resolves to a loopback address: 127.0.1.1; using 172.16.0.39 instead (on interface eth0)\n",
      "2024-02-20 18:09:41,508 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-02-20 18:09:43,340 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "2024-02-20 18:09:50,274 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "2024-02-20 18:09:50,284 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['YARN_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"yarn\") \\\n",
    "                    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "                    .config(\"spark.executor.cores\", 2) \\\n",
    "                    .config(\"spark.driver.cores\", 2) \\\n",
    "                    .appName(\"Learning DataFrames\") \\\n",
    "                    .config(\"spark.ui.port\", \"4051\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7414b00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "events = spark.read.json(\"/user/master/data/events/date=2022-05-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbf7a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event: struct (nullable = true)\n",
      " |    |-- admins: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- channel_id: long (nullable = true)\n",
      " |    |-- datetime: string (nullable = true)\n",
      " |    |-- media: struct (nullable = true)\n",
      " |    |    |-- media_type: string (nullable = true)\n",
      " |    |    |-- src: string (nullable = true)\n",
      " |    |-- message: string (nullable = true)\n",
      " |    |-- message_channel_to: long (nullable = true)\n",
      " |    |-- message_from: long (nullable = true)\n",
      " |    |-- message_group: long (nullable = true)\n",
      " |    |-- message_id: long (nullable = true)\n",
      " |    |-- message_to: long (nullable = true)\n",
      " |    |-- message_ts: string (nullable = true)\n",
      " |    |-- reaction_from: string (nullable = true)\n",
      " |    |-- reaction_type: string (nullable = true)\n",
      " |    |-- subscription_channel: long (nullable = true)\n",
      " |    |-- tags: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- user: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc991e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|message_from|lag_7 |\n",
      "+------------+------+\n",
      "|155747      |98478 |\n",
      "|155747      |23058 |\n",
      "|155747      |32788 |\n",
      "|155747      |121581|\n",
      "|155058      |37570 |\n",
      "|155058      |11144 |\n",
      "|155058      |139528|\n",
      "|155058      |94655 |\n",
      "|155058      |111506|\n",
      "|155058      |116979|\n",
      "+------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window().partitionBy('event.message_from').orderBy('event.datetime')\n",
    "\n",
    "dfWithLag = events.withColumn(\"lag_7\",F.lag(\"event.message_to\", 7).over(window))\n",
    "\n",
    "dfWithLag.select(\"event.message_from\", \"lag_7\") \\\n",
    ".filter(F.col(\"lag_7\").isNotNull()) \\\n",
    ".orderBy(F.col(\"event.message_from\").desc()) \\\n",
    ".show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c4d1245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|message_from|lag_7 |\n",
      "+------------+------+\n",
      "|155747      |98478 |\n",
      "|155747      |23058 |\n",
      "|155747      |32788 |\n",
      "|155747      |121581|\n",
      "|155058      |37570 |\n",
      "|155058      |11144 |\n",
      "|155058      |139528|\n",
      "|155058      |94655 |\n",
      "|155058      |111506|\n",
      "|155058      |116979|\n",
      "+------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Определение оконной спецификации: группировка по отправителю и сортировка по времени\n",
    "window = Window.partitionBy('event.message_from').orderBy('event.datetime')\n",
    "\n",
    "# Создание колонки lag_7 для определения, с кем пользователь взаимодействовал семь сообщений назад\n",
    "dfWithLag = events.withColumn(\"lag_7\", F.lag(\"event.message_to\", 7).over(window))\n",
    "\n",
    "# Фильтрация записей, где получатель (lag_7) не NULL, и сортировка по отправителю (message_from) в порядке убывания\n",
    "dfWithLag_filtered = dfWithLag.filter(F.col(\"lag_7\").isNotNull())\n",
    "\n",
    "dfWithLag_filtered.select(\n",
    "    F.col(\"event.message_from\"), \n",
    "    F.col(\"lag_7\")\n",
    ").orderBy(F.col(\"event.message_from\").desc()) \\\n",
    ".show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "033f254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - timefor timefor          0 2024-02-20 18:58 /user/timefor/data/events\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls \"/user/timefor/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "880e7a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "events = spark.read.json(\"/user/master/data/events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af338e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "events.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"event_type\") \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet(\"/user/timefor/data/events\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59efe52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+\n",
      "|               event|      date|  event_type|\n",
      "+--------------------+----------+------------+\n",
      "|[[19342], 987160,...|2022-05-31|     message|\n",
      "|[,, 2022-05-31 23...|2022-05-31|subscription|\n",
      "|[[26358], 247511,...|2022-05-31|     message|\n",
      "|[[79792], 748847,...|2022-05-31|     message|\n",
      "|[,, 2022-05-31 23...|2022-05-31|subscription|\n",
      "|[,, 2022-05-31 23...|2022-05-31|subscription|\n",
      "|[[151897], 396845...|2022-05-31|     message|\n",
      "|[,, 2022-05-31 23...|2022-05-31|subscription|\n",
      "|[,, 2022-05-31 23...|2022-05-31|subscription|\n",
      "|[,, 2022-05-31 23...|2022-05-31|subscription|\n",
      "+--------------------+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:====================================================>   (15 + 1) / 16]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Чтение данных из Parquet файла\n",
    "df = spark.read.parquet('/user/timefor/data/events')\n",
    "\n",
    "# Сортировка данных по времени в порядке убывания и вывод последних 10 строк\n",
    "df.orderBy(F.col('event.datetime').desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00fa30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351377d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c261c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12ca1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3199a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c88a7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cda6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
